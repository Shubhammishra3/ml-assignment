{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93949424-c371-4401-8731-6eca3a1a2327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"                          [assignment]\n",
    "1] Define overfitting and under fitting in machine learning. What are the consequence of each , how can be mitigeated. \n",
    "\n",
    "answer] Overfitting: A statistical model is said to be overfitted when the model does not make accurate predictions on testing\n",
    "data. When a model gets trained with so much data, it starts learning from the noise and inaccurate data entries in our data\n",
    "set. And when testing with test data results in High variance. Then the model does not categorize the data correctly, because\n",
    "of too many details and noise. \n",
    "                        Techniques to reduce overfitting:\n",
    "\n",
    "Increase training data.\n",
    "Reduce model complexity.\n",
    "Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to \n",
    "increase stop training).\n",
    "Ridge Regularization and Lasso Regularization\n",
    "Use dropout for neural networks to tackle overfitting.\n",
    "\n",
    "Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture the \n",
    "underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data\n",
    "                       Techniques to reduce underfitting: \n",
    "\n",
    "Increase model complexity\n",
    "Increase the number of features, performing feature engineering\n",
    "Remove noise from the data.\n",
    "Increase the number of epochs or increase the duration of training to get better results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c519749-b98d-42e7-8c08-62d96c780a57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de362bb7-db05-4481-98cc-2b9ef04d01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "2] How can we reduce overfitting? explain in brife.\n",
    "\n",
    "answer] Techniques to reduce overfitting:\n",
    "\n",
    "Increase training data.\n",
    "Reduce model complexity.\n",
    "Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to \n",
    "increase stop training).\n",
    "Ridge Regularization and Lasso Regularization\n",
    "Use dropout for neural networks to tackle overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc77d7f1-c07a-4e40-a53a-c2e1fca08433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a27090-ce18-46fa-830d-bbeed52de808",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"3] Explain under fitting? list senerioes where under fitting occur in ML.\n",
    "\n",
    "answer] Underfitting: A statistical model or a machine learning algorithm is said to have underfitting when it cannot capture  \n",
    "underlying trend of the data, i.e., it only performs well on training data but performs poorly on testing data\n",
    "Underfitting occurs when a model is too simple — informed by too few features or regularized too much — which makes it \n",
    "inflexible in learning from the dataset. Simple learners tend to have less variance in their predictions but more bias \n",
    "towards wrong outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ce9ec-a949-49ff-bb20-dde142153136",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485a0af2-ea36-4113-ac9a-3c98e29a1793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "4] Explain the bias variance trade off in machine learning.What is the relation ship between biase and variance and how do\n",
    "they affect model performance.\n",
    "\n",
    "answer] In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the \n",
    "parameter estimated across samples can be reduced by increasing the bias in the estimated parameters\n",
    "“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f2350b-497d-4987-b8bc-0c0e27ba0e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5845378d-1383-4bd6-921e-d847d32323c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "5] Discuss some common methos for decting overfitting and underfitting in machine learning models. How can you determine\n",
    "wheather your model is overfitting or under fitting ?\n",
    "\n",
    "answer] We can determine whether a predictive model is underfitting or overfitting the training data by looking at the \n",
    "prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model \n",
    "performs poorly on the training data.\n",
    "One way to detect underfitting situation is to use the bias-variance approach, which can be represented like this:\n",
    "\n",
    "Your model is under fitted when you have a high bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03acc62f-9872-4934-a20d-902979725ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428cb552-cf75-4ab1-849f-51d862c489f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "6] Compare and contrast bias and variance in machine learning . What are some examples of high bias and variance models and\n",
    "how do they differ in terms of performance?\n",
    "\n",
    "answer]Bias: Assumptions made by a model to make a function easier to learn. It is actually the error rate of the training data\n",
    ". When the error rate has a high value, we call it High Bias and when the error rate has a low value, we call it low Bias.\n",
    "\n",
    "Variance:  The difference between the error rate of training data and testing data is called variance. If the difference is\n",
    "high then it’s called high variance and when the difference of errors is low then it’s called low variance. Usually, we want\n",
    "to make a low variance for generalized our model.\n",
    "High Bias - High Variance: Predictions are inconsistent and inaccurate on average. Low Bias - Low Variance: It is an ideal \n",
    "model. But, we cannot achieve this. Low Bias - High Variance (Overfitting): Predictions are inconsistent and accurate on \n",
    "average.\n",
    "Examples of high-bias machine learning algorithms include: Linear Regression, Linear Discriminant Analysis and\n",
    "Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ef307b-8751-4bd2-807e-27547d987fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0023c5-89d8-4538-a749-5723443ee0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "7] What is regularization in machine learning , how can it be used to prevent overfitting ? Describe some common techniques \n",
    "of regularization and how they work.\n",
    "\n",
    "ANSWER]  Regularization refers to techniques that are used to calibrate machine learning models in order to minimize the\n",
    "adjusted loss function and prevent overfitting or underfitting.\n",
    "L1 Regularization, also called a lasso regression, adds the “absolute value of magnitude” of the coefficient as a penalty\n",
    "term to the loss function. L2 Regularization, also called a ridge regression, adds the “squared magnitude” of the coefficient \n",
    "as the penalty term to the loss function.\n",
    "Regularization works by adding a penalty or complexity term or shrinkage term with Residual Sum of Squares (RSS) to the\n",
    "complex model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c42b606-efc0-4ad7-b639-43e8148f7294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064d99e2-671f-4fd3-9c3b-9672dbcf917c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
